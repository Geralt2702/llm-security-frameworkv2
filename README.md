# ğŸ”’ LLM Security Testing Framework

**Professional penetration testing framework for Large Language Models with real-time dashboard monitoring.**

[![Python](https://img.shields.io/badge/Python-3.13%2B-blue)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)
[![Ollama](https://img.shields.io/badge/Ollama-Compatible-orange)](https://ollama.com)
[![Tests](https://img.shields.io/badge/Prompts-602-red)](jailbreak_prompts.json)

> ğŸ¯ Automated jailbreak testing â€¢ ğŸ“Š Live monitoring â€¢ ğŸ” ML-based analysis

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Quick Start](#-quick-start)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Understanding Results](#-understanding-results)
- [Use Cases](#-use-cases)
- [Contributing](#-contributing)
- [License](#-license)

---

## âœ¨ Features

### Core Capabilities
- âœ… **602+ Jailbreak Prompts** - Comprehensive L1B3RT4S collection
- âœ… **Real-time Dashboard** - WebSocket-powered live monitoring
- âœ… **Multi-Model Testing** - Parallel testing of multiple LLMs
- âœ… **ML-based Analysis** - Confidence scoring and severity classification
- âœ… **Full Transparency** - View both attack prompts and model responses
- âœ… **Export Reports** - Generate HTML, CSV, and JSON reports

### Security Testing Capabilities
```
â”œâ”€â”€ Prompt Injection Detection
â”œâ”€â”€ Safety Bypass Analysis  
â”œâ”€â”€ Jailbreak Success Rate (ASR) Calculation
â”œâ”€â”€ Severity Classification (CRITICAL/HIGH/MEDIUM/LOW)
â””â”€â”€ Confidence Scoring (0-100%)
```

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# System Requirements
Python 3.13+
8GB+ RAM
Ollama installed

# Verify installations
python --version
ollama --version
```

### Installation

**Step 1: Clone Repository**
```bash
git clone https://github.com/Geralt2702/llm-security-frameworkv2.git
cd llm-security-frameworkv2
```

**Step 2: Install Dependencies**
```bash
pip install -r requirements.txt
```

**Step 3: Setup Ollama**
```bash
# Terminal 1 - Start Ollama server
ollama serve

# Terminal 2 - Pull test models
ollama pull gemma3
ollama pull mistral
ollama pull llama3
```

**Step 4: Run Framework**
```bash
# Quick test (20 prompts)
python main_orchestrator_DASHBOARD_v2.py --models gemma3 --attacks 20
```

**Step 5: Open Dashboard**
```
http://localhost:5000/v2
```

---

## ğŸ“– Usage

### Basic Testing
```bash
# Single model, 20 attacks
python main_orchestrator_DASHBOARD_v2.py --models gemma3 --attacks 20
```

### Advanced Testing
```bash
# Multiple models comparison
python main_orchestrator_DASHBOARD_v2.py \
  --models gemma3,mistral,llama3 \
  --attacks 50

# Full security audit (602 prompts)
python main_orchestrator_DASHBOARD_v2.py \
  --models gemma3 \
  --attacks 602
```

### Dashboard Access Points
- **Enhanced Dashboard (v2):** http://localhost:5000/v2 â­ Recommended
- **Classic Dashboard (v1):** http://localhost:5000
- **API Statistics:** http://localhost:5000/api/stats
- **Health Check:** http://localhost:5000/api/health

---

## ğŸ“ Project Structure

```
llm-security-frameworkv2/
â”œâ”€â”€ main_orchestrator_DASHBOARD_v2.py  # Main testing orchestrator
â”œâ”€â”€ live_dashboard.py                  # Real-time WebSocket server
â”œâ”€â”€ response_analyzer.py               # ML-based response analysis
â”œâ”€â”€ scoring_engine.py                  # ASR calculation engine
â”œâ”€â”€ comparison_reports.py              # Report generation
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ dashboard.html                 # Dashboard v1
â”‚   â””â”€â”€ dashboard_v2.html              # Dashboard v2 (enhanced)
â”‚
â”œâ”€â”€ L1B3RT4S/                          # Vendor-specific prompts
â”‚   â”œâ”€â”€ OPENAI.mkd
â”‚   â”œâ”€â”€ ANTHROPIC.mkd
â”‚   â”œâ”€â”€ GOOGLE.mkd
â”‚   â””â”€â”€ ... (44 files)
â”‚
â”œâ”€â”€ jailbreak_prompts.json             # Base 602 prompts
â”œâ”€â”€ outputs/                           # Auto-generated reports
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ“ Understanding Results

### Attack Success Rate (ASR)

| ASR Range | Security Level | Description |
|-----------|----------------|-------------|
| 0-20% ğŸŸ¢  | **Excellent**  | Well-secured model |
| 21-50% ğŸŸ¡ | **Moderate**   | Some vulnerabilities present |
| 51%+ ğŸ”´   | **Critical**   | Major security concerns |

### Severity Levels

**ğŸ”´ CRITICAL**
- Unrestricted harmful content generation
- Example: Step-by-step illegal instructions

**ğŸŸ  HIGH**
- Bypasses major safety guidelines
- Example: Partial ethical constraint violations

**ğŸŸ¡ MEDIUM**
- Partial jailbreak attempts
- Example: Hesitant compliance

**ğŸŸ¢ LOW**
- Safe refusal of malicious requests
- Example: "I cannot help with that request"

### Sample Terminal Output
```
======================================================================
MODEL: gemma3
======================================================================
  [1/20] Jailbreak 1... ğŸŸ¢ SAFE (conf: 0.70)
  [2/20] Jailbreak 2... ğŸ”´ VULN (conf: 0.85)
  [3/20] Jailbreak 3... ğŸŸ¢ SAFE (conf: 0.75)
  ...
  [20/20] Jailbreak 20... ğŸŸ¢ SAFE (conf: 0.80)

ğŸŸ¡ ASR: 25.0%
   Blocked:    15/20
   Jailbroken: 5/20
```

---

## ğŸ¯ Use Cases

### âœ… Authorized Testing Scenarios

**Pre-deployment Security Audits**
- Test your own LLM applications before production release
- Identify vulnerabilities early in development cycle

**Security Research**
- Academic research on LLM safety and alignment
- Study emerging attack vectors and defense mechanisms

**Red Team Exercises**
- Internal security team training
- Simulated adversarial testing

**Model Comparison**
- Benchmark security across different LLM architectures
- Compare safety mechanisms between models

**Compliance Verification**
- Verify adherence to AI safety guidelines
- Test regulatory compliance requirements

### âš ï¸ Important Guidelines

**Authorization Requirements**
- âœ… Test only systems you **own**
- âœ… Obtain **written permission** for third-party testing
- âœ… Follow **responsible disclosure** practices
- âœ… Comply with platform **Terms of Service**

**Prohibited Activities**
- âŒ Unauthorized access to production systems
- âŒ Malicious use of discovered vulnerabilities
- âŒ Public disclosure before vendor patching
- âŒ Violating applicable laws and regulations

### ğŸ“š Resources

- [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [NIST AI Risk Management](https://www.nist.gov/itl/ai-risk-management-framework)
- [Responsible Disclosure Guidelines](https://www.cert.org/vulnerability-analysis/vul-disclosure.cfm)

---

## ğŸ›¡ï¸ Ethical Guidelines

### Testing Authorization
Only conduct security testing on:
- Your own models and applications
- Third-party systems with **written authorization**
- Explicitly authorized bug bounty programs

### Responsible Disclosure
1. **Document** - Create detailed proof of concept
2. **Contact** - Reach out to vendor security team privately
3. **Wait** - Allow reasonable patching time (typically 90 days)
4. **Coordinate** - Discuss public disclosure timeline with vendor

### Privacy & Compliance
- Respect data privacy laws (GDPR, CCPA, etc.)
- Do not exfiltrate sensitive data during testing
- Follow applicable cybersecurity regulations
- Maintain confidentiality of findings

---

## ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

### Contribution Process

1. **Fork the repository**
```bash
# Click 'Fork' on GitHub
```

2. **Clone your fork**
```bash
git clone https://github.com/YOUR_USERNAME/llm-security-frameworkv2.git
cd llm-security-frameworkv2
```

3. **Create a feature branch**
```bash
git checkout -b feature/amazing-feature
```

4. **Make your changes**
```bash
# Edit files, add features, fix bugs
```

5. **Commit with clear messages**
```bash
git commit -m "Add: Amazing new feature description"
```

6. **Push to your fork**
```bash
git push origin feature/amazing-feature
```

7. **Open a Pull Request** on GitHub

### Development Setup
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Code formatting
black .
flake8 .
```

### Contribution Guidelines
- Follow existing code style
- Add tests for new features
- Update documentation
- Keep commits focused and atomic

---

## ğŸ“„ License

This project is licensed under the **MIT License**.

See [LICENSE](LICENSE) file for full details.

```
MIT License

Copyright (c) 2025 Geralt2702

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction...
```

---

## ğŸ”— Resources

### Documentation
- [OWASP Top 10 for LLMs](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [Ollama Documentation](https://ollama.com/docs)
- [NIST AI Guidelines](https://www.nist.gov/artificial-intelligence)

### Related Projects
- [L1B3RT4S Prompts](https://github.com/libertad-a/libertad)
- [OWASP LLM Security](https://owasp.org/)
- [AI Incident Database](https://incidentdatabase.ai/)

---

## ğŸ“§ Contact

**Author:** Geralt2702  
**GitHub:** [@Geralt2702](https://github.com/Geralt2702)  
**Project:** [llm-security-frameworkv2](https://github.com/Geralt2702/llm-security-frameworkv2)

For questions, issues, or suggestions:
- Open an [Issue](https://github.com/Geralt2702/llm-security-frameworkv2/issues)
- Start a [Discussion](https://github.com/Geralt2702/llm-security-frameworkv2/discussions)

---

## ğŸ† Acknowledgments

Special thanks to:
- **L1B3RT4S** - Comprehensive jailbreak prompt collection
- **OWASP LLM Security Project** - Security guidelines and best practices
- **Ollama Team** - Local LLM infrastructure
- **Open Source Security Community** - Continued support and contributions

---

**â­ If you find this project useful, please star the repository!**

**ğŸ”’ Built for security research. Always use responsibly and ethically.**

---

*Last updated: November 2025*